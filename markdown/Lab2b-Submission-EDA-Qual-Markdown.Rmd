---
title: "Business Intelligence Lab Submission Markdown"
author: "Acers Team"
date: "8th October 2023"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *Student ID Numbers and Names of Group Members* | \<list one student name, class group (just the letter; A, B, or C), and ID per line, e.g., 123456 - A - John Leposo; you should be between 2 and 5 members per group\> |
|                                                   |                                                                                                                                                                          |
|                                                   | 1.  122790 - C - Nicholas Bwalley                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 2.  133834 - C - Sarah Mongare                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 3.  134879 - C - Lesley Tulienge                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 4.  124461 - C - Angela Kinya                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 5.  133928 - C - Millicent Cheptoi                                                                                                                                             |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *GitHub Classroom Group Name*                   | acers_team                                                                                                                                                               |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *Course Code*                                   | BBT4206                                                                                                                                                                  |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *Course Name*                                   | Business Intelligence II                                                                                                                                                 |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *Program*                                       | Bachelor of Business Information Technology                                                                                                                              |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *Semester Duration*                             | 21^st^ August 2023 to 28^th^ November 2023                                                                                                                               |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

# Setup Chunk

We start by installing all the required packages

{r Install Packages, echo=TRUE, message=FALSE, warning=FALSE}
## formatR - Required to format R code in the markdown ----
if (!is.element("formatR", installed.packages()[, 1])) {
  install.packages("formatR", dependencies = TRUE,
                   repos="https://cloud.r-project.org")
}
require("formatR")

## dplyr - For data manipulation ----
if (!is.element("dplyr", installed.packages()[, 1])) {
  install.packages("dplyr", dependencies = TRUE,
  repos = "https://cloud.r-project.org")
}
require("dplyr")

## ggplot2 - For data visualizations using the Grammar for Graphics package ----
if (!is.element("ggplot2", installed.packages()[, 1])) {
install.packages("ggplot2", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("ggplot2")

## ggrepel - Additional options for the Grammar for Graphics package ----
if (!is.element("ggrepel", installed.packages()[, 1])) {
install.packages("ggrepel", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("ggrepel")

## ggraph - Additional options for the Grammar for Graphics package ----
if (!is.element("ggraph", installed.packages()[, 1])) {
install.packages("ggraph", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("ggraph")

## tidytext - For text mining ----
if (!is.element("tidytext", installed.packages()[, 1])) {
install.packages("tidytext", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("tidytext")

## tidyr - To tidy messy data ----
if (!is.element("tidyr", installed.packages()[, 1])) {
install.packages("tidyr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("tidyr")

## widyr - To widen, process, and re-tidy a dataset ----
if (!is.element("widyr", installed.packages()[, 1])) {
install.packages("widyr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("widyr")

## gridExtra - to arrange multiple grid-based plots on a page ----
if (!is.element("gridExtra", installed.packages()[, 1])) {
install.packages("gridExtra", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("gridExtra")

## knitr - for dynamic report generation ----
if (!is.element("knitr", installed.packages()[, 1])) {
install.packages("knitr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("knitr")

## kableExtra - for nicely formatted output tables ----
if (!is.element("kableExtra", installed.packages()[, 1])) {
install.packages("kableExtra", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("kableExtra")

## formattable -  To create a formattable object ----
# A formattable object is an object to which a formatting function and related
# attributes are attached.
if (!is.element("formattable", installed.packages()[, 1])) {
install.packages("formattable", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("formattable")

## circlize - To create a cord diagram or visualization ----
# by Gu et al. (2014)
if (!is.element("circlize", installed.packages()[, 1])) {
install.packages("circlize", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("circlize")

## memery - For creating data analysis related memes ----
# The memery package generates internet memes that optionally include a
# superimposed inset plot and other atypical features, combining the visual
# impact of an attention-grabbing meme with graphic results of data analysis.
if (!is.element("memery", installed.packages()[, 1])) {
install.packages("memery", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("memery")

## magick - For image processing in R ----
if (!is.element("magick", installed.packages()[, 1])) {
install.packages("magick", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("magick")

## yarrr - To create a pirate plot ----
if (!is.element("yarrr", installed.packages()[, 1])) {
install.packages("yarrr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("yarrr")

## radarchart - To create interactive radar charts using ChartJS ----
if (!is.element("radarchart", installed.packages()[, 1])) {
install.packages("radarchart", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("radarchart")

## igraph - To create ngram network diagrams ----
if (!is.element("igraph", installed.packages()[, 1])) {
install.packages("igraph", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("igraph")

## wordcloud2 - For creating wordcloud by using 'wordcloud2.JS ----
if (!is.element("wordcloud2", installed.packages()[, 1])) {
install.packages("wordcloud2", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("wordcloud2")

## readr - Load datasets from CSV files ----
if (!is.element("readr", installed.packages()[, 1])) {
install.packages("readr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
require("readr")

## textstem - Used to lemmatize words ----
if (!is.element("textstem", installed.packages()[, 1])) {
  install.packages("textstem", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("textstem")

## hunspell - High-Performance Stemmer, Tokenizer, and Spell Checker ----
if (!is.element("hunspell", installed.packages()[, 1])) {
  install.packages("hunspell", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("hunspell")

## stringr - For processing characters in a string ----
if (!is.element("stringr", installed.packages()[, 1])) {
  install.packages("stringr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("stringr")


------------------------------------------------------------------------

*Note:* the following "KnitR" options have been set as the defaults in this markdown:\
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy.opts = list(width.cutoff = 80), tidy = TRUE).

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

{r setup, echo=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	eval = TRUE,
	echo = TRUE,
	warning = FALSE,
	collapse = FALSE,
	tidy = TRUE
)


------------------------------------------------------------------------

*Note:* the following "R Markdown" options have been set as the defaults in this markdown:

> output:\
> \
> github_document:\
> toc: yes\
> toc_depth: 4\
> fig_width: 6\
> fig_height: 4\
> df_print: default\
> \
> editor_options:\
> chunk_output_type: console

# Customize Visualizations, Tables and Colour Schemes
{r customize}
# The following defines a blue-grey colour scheme for the visualizations:
## shades of blue and shades of grey
blue_grey_colours_11 <- c("#27408E", "#304FAF", "#536CB5", "#6981c7", "#8da0db",
                          "#dde5ec", "#c8c9ca", "#B9BCC2", "#A7AAAF", "#888A8E",
                          "#636569")

blue_grey_colours_6 <- c("#27408E", "#304FAF", "#536CB5",
                         "#B9BCC2", "#A7AAAF", "#888A8E")

blue_grey_colours_4 <- c("#27408E", "#536CB5",
                         "#B9BCC2", "#888A8E")

blue_grey_colours_3 <- c("#6981c7", "#304FAF", "#888A8E")

blue_grey_colours_2 <- c("#27408E",
                         "#888A8E")

blue_grey_colours_1 <- c("#6981c7")

# Custom theme for visualizations
blue_grey_theme <- function() {
  theme(
    axis.ticks = element_line(
                              linewidth = 1, linetype = "dashed",
                              lineend = NULL, color = "#dfdede",
                              arrow = NULL, inherit.blank = FALSE),
    axis.text = element_text(
                             face = "bold", color = "#3f3f41",
                             size = 12, hjust = 0.5),
    axis.title = element_text(face = "bold", color = "#3f3f41",
                              size = 14, hjust = 0.5),
    plot.title = element_text(face = "bold", color = "#3f3f41",
                              size = 16, hjust = 0.5),
    panel.grid = element_line(
                              linewidth = 0.1, linetype = "dashed",
                              lineend = NULL, color = "#dfdede",
                              arrow = NULL, inherit.blank = FALSE),
    panel.background = element_rect(fill = "#f3eeee"),
    legend.title = element_text(face = "plain", color = "#3f3f41",
                                size = 12, hjust = 0),
    legend.position = "right"
  )
}

# Customize the text tables for consistency using HTML formatting
kable_theme <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
    kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                  full_width = FALSE)
}


# Loading the Student Performance Dataset

The 20230412-20230719-BI1-BBIT4-1-StudentPerformanceDataset is then loaded. The dataset and its metadata are available here: <https://drive.google.com/drive/folders/1-BGEhfOwquXF6KKXwcvrx7WuZXuqmW9q?usp=sharing>

{r Load Dataset}
student_performance_dataset <-
  readr::read_csv(
                  "../data/20230412-20230719-BI1-BBIT4-1-StudentPerformanceDataset.CSV",
                  col_types =
                  readr::cols(
                              class_group =
                              readr::col_factor(levels = c("A", "B", "C")),
                              gender = readr::col_factor(levels = c("1", "0")),
                              YOB = readr::col_date(format = "%Y"),
                              regret_choosing_bi =
                              readr::col_factor(levels = c("1", "0")),
                              drop_bi_now =
                              readr::col_factor(levels = c("1", "0")),
                              motivator =
                              readr::col_factor(levels = c("1", "0")),
                              read_content_before_lecture =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              anticipate_test_questions =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              answer_rhetorical_questions =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              find_terms_I_do_not_know =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              copy_new_terms_in_reading_notebook =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              take_quizzes_and_use_results =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              reorganise_course_outline =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              write_down_important_points =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              space_out_revision =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              studying_in_study_group =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              schedule_appointments =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              goal_oriented =
                              readr::col_factor(levels =
                                                c("1", "0")),
                              spaced_repetition =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              testing_and_active_recall =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              interleaving =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              categorizing =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              retrospective_timetable =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              cornell_notes =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4")),
                              sq3r = readr::col_factor(levels =
                                                       c("1", "2", "3", "4")),
                              commute = readr::col_factor(levels =
                                                          c("1", "2",
                                                            "3", "4")),
                              study_time = readr::col_factor(levels =
                                                             c("1", "2",
                                                               "3", "4")),
                              repeats_since_Y1 = readr::col_integer(),
                              paid_tuition = readr::col_factor(levels =
                                                               c("0", "1")),
                              free_tuition = readr::col_factor(levels =
                                                               c("0", "1")),
                              extra_curricular = readr::col_factor(levels =
                                                                   c("0", "1")),
                              sports_extra_curricular =
                              readr::col_factor(levels = c("0", "1")),
                              exercise_per_week = readr::col_factor(levels =
                                                                    c("0", "1",
                                                                      "2",
                                                                      "3")),
                              meditate = readr::col_factor(levels =
                                                           c("0", "1",
                                                             "2", "3")),
                              pray = readr::col_factor(levels =
                                                       c("0", "1",
                                                         "2", "3")),
                              internet = readr::col_factor(levels =
                                                           c("0", "1")),
                              laptop = readr::col_factor(levels = c("0", "1")),
                              family_relationships =
                              readr::col_factor(levels =
                                                c("1", "2", "3", "4", "5")),
                              friendships = readr::col_factor(levels =
                                                              c("1", "2", "3",
                                                                "4", "5")),
                              romantic_relationships =
                              readr::col_factor(levels =
                                                c("0", "1", "2", "3", "4")),
                              spiritual_wellnes =
                              readr::col_factor(levels = c("1", "2", "3",
                                                           "4", "5")),
                              financial_wellness =
                              readr::col_factor(levels = c("1", "2", "3",
                                                           "4", "5")),
                              health = readr::col_factor(levels = c("1", "2",
                                                                    "3", "4",
                                                                    "5")),
                              day_out = readr::col_factor(levels = c("0", "1",
                                                                     "2", "3")),
                              night_out = readr::col_factor(levels = c("0",
                                                                       "1", "2",
                                                                       "3")),
                              alcohol_or_narcotics =
                              readr::col_factor(levels = c("0", "1", "2", "3")),
                              mentor = readr::col_factor(levels = c("0", "1")),
                              mentor_meetings = readr::col_factor(levels =
                                                                  c("0", "1",
                                                                    "2", "3")),
                              `Attendance Waiver Granted: 1 = Yes, 0 = No` =
                              readr::col_factor(levels = c("0", "1")),
                              GRADE = readr::col_factor(levels =
                                                        c("A", "B", "C", "D",
                                                          "E"))),
                  locale = readr::locale())


## Description of the Dataset

We then display the number of observations and number of variables. We have 101 observations and 100 variables to work with.

{r Your Fourth Code Chunk}
dim(student_performance_dataset)


Next, we display the quartiles for each numeric variable[*... think of this process as **"storytelling using the data."*]Here, a summary of each variable in the dataset is displayed. It includes the minimum, 1st quartile, median, mean, 3rd quartile and the maximum.{#highlight style="color: blue"}

{r Your Fifth Code Chunk}
summary(student_performance_dataset)


## Data Manipulation
The dplyr package allows to create a subset of the data and provides a set of verbs to help solve data manipulation challenges. These verbs include: mutate, filter, group_by, select,summarize and arrange.More documentation on dplyr can be found here: <https://cran.r-project.org/package=dplyr> or <https://github.com/tidyverse/dplyr>

There is also the pipe operator, %>% ,that chains operations together to enhance readability and maintainability of code. The %>% operator takes the result of the expression on its left and passes it as the first argument to the function on its right. This allows you to chain together a sequence of operations on a dataset or object.

{r Your Sixth Code Chunk}

evaluation_per_group_per_gender <- student_performance_dataset %>% 
  mutate(`Student's Gender` =
           ifelse(gender == 1, "Male", "Female")) %>%
  select(class_group, gender,
         `Student's Gender`, `Average Course Evaluation Rating`) %>%
  filter(!is.na(`Average Course Evaluation Rating`)) %>%
  group_by(class_group, `Student's Gender`) %>%
  summarise(average_evaluation_rating =
              mean(`Average Course Evaluation Rating`)) %>%
  arrange(desc(average_evaluation_rating), .by_group = TRUE)
View(evaluation_per_group_per_gender)

evaluation_per_group_per_gender %>%
  rename(`Class Group` = class_group) %>%
  rename(`Average Course Evaluation Rating` = average_evaluation_rating) %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`) %>%
  mutate(`Average Course Evaluation Rating` =
           color_tile("#B9BCC2", "#536CB5")
           (`Average Course Evaluation Rating`)) %>%
  kable("html", escape = FALSE, align = "c",
        caption = "Course Evaluation Rating per Group and per Gender") %>%
  kable_styling(bootstrap_options =
                  c("striped", "condensed", "bordered"),
                full_width = FALSE)
evaluation_per_group_per_gender %>%
  ggplot() +
  geom_bar(aes(x = class_group, y = average_evaluation_rating,
               fill = `Student's Gender`),
           stat = "identity", position = "dodge") +
  expand_limits(y = 0) +
  blue_grey_theme() +
  scale_fill_manual(values = blue_grey_colours_2) +
  ggtitle("Course Evaluation Rating per Group and per Gender") +
  labs(x = "Class Group", y = "Average Rating")


## Data Cleansing

The cleansing for qualitative data includes: contractions,removal of special characters,putting in lower case to standardize the text, stemming-generally refers to removing suffixes from words to get the common origin,lemmatization-reducing inflected (or sometimes derived) words to their word stem, base or root form, tokenization and stopword removal.

{r Your Seventh Code Chunk}
expand_contractions <- function(doc) {
  doc <- gsub("I'm", "I am", doc, ignore.case = TRUE)
  doc <- gsub("you're", "you are", doc, ignore.case = TRUE)
  doc <- gsub("he's", "he is", doc, ignore.case = TRUE)
  doc <- gsub("she's", "she is", doc, ignore.case = TRUE)
  doc <- gsub("it's", "it is", doc, ignore.case = TRUE)
  doc <- gsub("we're", "we are", doc, ignore.case = TRUE)
  doc <- gsub("they're", "they are", doc, ignore.case = TRUE)
  doc <- gsub("I'll", "I will", doc, ignore.case = TRUE)
  doc <- gsub("you'll", "you will", doc, ignore.case = TRUE)
  doc <- gsub("he'll", "he will", doc, ignore.case = TRUE)
  doc <- gsub("she'll", "she will", doc, ignore.case = TRUE)
  doc <- gsub("it'll", "it will", doc, ignore.case = TRUE)
  doc <- gsub("we'll", "we will", doc, ignore.case = TRUE)
  doc <- gsub("they'll", "they will", doc, ignore.case = TRUE)
  doc <- gsub("won't", "will not", doc, ignore.case = TRUE)
  doc <- gsub("can't", "cannot", doc, ignore.case = TRUE)
  doc <- gsub("n't", " not", doc, ignore.case = TRUE)
  return(doc)
}

# Evaluation likes and wishes
evaluation_likes_and_wishes <- student_performance_dataset %>%
  mutate(`Student's Gender` =
           ifelse(gender == 1, "Male", "Female")) %>%
  rename(`Class Group` = class_group) %>%
  rename(Likes = `D - 1. \nWrite two things you like about the teaching and learning in this unit so far.`) %>% 
  rename(Wishes = `D - 2. Write at least one recommendation to improve the teaching and learning in this unit (for the remaining weeks in the semester)`) %>% 
  select(`Class Group`,
         `Student's Gender`, `Average Course Evaluation Rating`,
         Likes, Wishes) %>%
  filter(!is.na(`Average Course Evaluation Rating`)) %>%
  arrange(`Class Group`)

# Before expanding contractions (See row number 4)
View(evaluation_likes_and_wishes)

evaluation_likes_and_wishes$Likes <- sapply(evaluation_likes_and_wishes$Likes, expand_contractions) 
evaluation_likes_and_wishes$Wishes <- sapply(evaluation_likes_and_wishes$Wishes, expand_contractions) 

# After expanding contractions
View(evaluation_likes_and_wishes)

## Special Characters and Lower Case ----
remove_special_characters <- function(doc) {
  gsub("[^a-zA-Z0-9 ]", "", doc, ignore.case = TRUE)
}

# Before removing special characters (See row number 11)
View(evaluation_likes_and_wishes)

evaluation_likes_and_wishes$Likes <- sapply(evaluation_likes_and_wishes$Likes, remove_special_characters) # nolint
evaluation_likes_and_wishes$Wishes <- sapply(evaluation_likes_and_wishes$Wishes, remove_special_characters) # nolint

# Convert everything to lower case (to standardize the text)
evaluation_likes_and_wishes$Likes <- sapply(evaluation_likes_and_wishes$Likes, tolower) # nolint
evaluation_likes_and_wishes$Wishes <- sapply(evaluation_likes_and_wishes$Wishes, tolower) # nolint

# After removing special characters and converting everything to lower case
View(evaluation_likes_and_wishes)

# [OPTIONAL] You can save the file as a CSV at this point
write.csv(evaluation_likes_and_wishes,
          file = "data/evaluation_likes_and_wishes.csv",
          row.names = FALSE)


# Additional examples can be seen here:
head(sample(stop_words$word, 20), 20)

# You can also create a list of words that you would like to censor
undesirable_words <- c("wow", "lol", "none", "na")

evaluation_likes_filtered <- evaluation_likes_and_wishes %>% # nolint
  # We start by tokenization (un-nesting words). This is from the variable
  # "Like" into the variable "word".
  unnest_tokens(word, Likes) %>%
  # Then we remove stopwords using an anti-join (remember this from the
  # BBT3104: Advanced Database Systems course)
  # Anti-join: do not join where the word is in the list of stopwords
  anti_join(stop_words, by = c("word")) %>%
  distinct() %>%
  # Censor or filter out unwanted words
  filter(!word %in% undesirable_words) %>%
  # Include only words that are more than 3 characters long (assuming that
  # these are the words that are meaningful)
  filter(nchar(word) > 3) %>%
  # We then rename the variable "word" for ease of use.
  rename(`Likes (tokenized)` = word) %>%
  # We focus only on the likes in this data frame
  select(-Wishes)

# Lastly, we save the created data frame as a CSV file:
write.csv(evaluation_likes_filtered,
          file = "data/evaluation_likes_filtered.csv",
          row.names = FALSE)

# The same is done to create a data frame for the "wishes" only
evaluation_wishes_filtered <- evaluation_likes_and_wishes %>% # nolint
  unnest_tokens(word, Wishes) %>%
  anti_join(stop_words, by = c("word")) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Wishes (tokenized)` = word) %>%
  select(-Likes)

write.csv(evaluation_wishes_filtered,
          file = "data/evaluation_wishes_filtered.csv",
          row.names = FALSE)

### Correct Spelling Mistakes ----
# We can correct spelling mistakes before lemmatization as follows:
# Define a custom function to correct spelling
correct_spelling <- function(x) {
  sapply(1:length(x),
    function(y) {
      bad <- hunspell(x[y])[[1]]
      good <-
        unlist(lapply(hunspell_suggest(bad),
                      `[[`, 1))

      if (length(bad)) {
        for (i in 1:length(bad)){
          x[y] <<- gsub(bad[i], good[i], x[y])
        }
      }
    }
  )
  x
}

#### Likes ----
# Before spelling correction
# Take note of the following misspelt words (for the trigram):
#   Line 40: "applicationsthe" instead of "applications the" # nolint
#   Line 102: "intergration" instead of "integration" # nolint
#   etc.
View(evaluation_likes_filtered)

corrected_spelling <-
  evaluation_likes_filtered$`Likes (tokenized)` %>%
  correct_spelling()

evaluation_likes_filtered$`Likes (tokenized)` <- corrected_spelling

# After spelling correction
View(evaluation_likes_filtered)

### Repeat the pre-processing for the correctly spelt words ----
# This is done for the sake of the words which were split into more than one
# word after correcting the spelling mistake.

# The repeated pre-processing includes:
# 1. Expanding Contractions
evaluation_likes_filtered$`Likes (tokenized)` <- sapply(evaluation_likes_filtered$`Likes (tokenized)`, expand_contractions) # nolint
# 2. Remove special Characters
evaluation_likes_filtered$`Likes (tokenized)` <- sapply(evaluation_likes_filtered$`Likes (tokenized)`, remove_special_characters) # nolint
# 3. Convert to Lower-Case for a standard form
evaluation_likes_filtered$`Likes (tokenized)` <- sapply(evaluation_likes_filtered$`Likes (tokenized)`, tolower) # nolint

# 4. Tokenization, stopword removal, short word removal, and censorship
evaluation_likes_filtered <- evaluation_likes_filtered %>% # nolint
  unnest_tokens(word, `Likes (tokenized)`, token = "ngrams", n = 1) %>%
  anti_join(stop_words, by = c("word")) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Likes (tokenized)` = word)

write.csv(evaluation_likes_filtered,
          file = "data/evaluation_likes_filtered.csv",
          row.names = FALSE)

View(evaluation_likes_filtered)

#### Wishes ----
# Before spelling correction
# Take note of the following misspelt words (for the trigram):
#   Line 40: "applicationsthe" instead of "applications the" # nolint
#   Line 102: "intergration" instead of "integration" # nolint
#   etc.
View(evaluation_wishes_filtered)

corrected_spelling <-
  evaluation_wishes_filtered$`Wishes (tokenized)` %>%
  correct_spelling()

evaluation_wishes_filtered$`Wishes (tokenized)` <- corrected_spelling

# After spelling correction
View(evaluation_wishes_filtered)

### Repeat the pre-processing for the correctly spelt words ----
# This is done for the sake of the words which were split into more than one
# word after correcting the spelling mistake.

# The repeated pre-processing includes:
# 1. Expanding Contractions
evaluation_wishes_filtered$`Wishes (tokenized)` <- sapply(evaluation_wishes_filtered$`Wishes (tokenized)`, expand_contractions) # nolint
# 2. Remove special Characters
evaluation_wishes_filtered$`Wishes (tokenized)` <- sapply(evaluation_wishes_filtered$`Wishes (tokenized)`, remove_special_characters) # nolint
# 3. Convert to Lower-Case for a standard form
evaluation_wishes_filtered$`Wishes (tokenized)` <- sapply(evaluation_wishes_filtered$`Wishes (tokenized)`, tolower) # nolint

# 4. Tokenization, stopword removal, short word removal, and censorship
evaluation_wishes_filtered <- evaluation_wishes_filtered %>% # nolint
  unnest_tokens(word, `Wishes (tokenized)`, token = "ngrams", n = 1) %>%
  anti_join(stop_words, by = c("word")) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Wishes (tokenized)` = word)

write.csv(evaluation_wishes_filtered,
          file = "data/evaluation_wishes_filtered.csv",
          row.names = FALSE)

View(evaluation_wishes_filtered)

### We can now perform lemmatization on the correctly spelt words ----
# We need to first create a lemma lookup table. The lemmatize_strings()
# function will then use this lookup table to replace the words.

# Think of the lemma lookup table as a subset of an entire lemma dictionary.
# This makes it easier to search for a word in the smaller lookup table
# instead of searching for a word in the entire lemma dictionary.

# Lemma lookup tables specific to a project can be made by referring to
# a pre-existing lemma dictionary. The code below shows how the "Hunspell"
# lemma dictionary is used to create  the lemma lookup table (subset of
# the dictionary).

# Lemma dictionaries include:
#   1. Hunspell: https://hunspell.github.io/ or
#                https://cran.r-project.org/package=hunspell
#   2. koRpus: https://reaktanz.de/?c=hacking&s=koRpus or
#              https://cran.r-project.org/package=koRpus
#   3. Michal MÄ›chura (2016): https://www.lexiconista.com/
#   etc.

lemma_dictionary_for_likes <-
  make_lemma_dictionary(evaluation_likes_filtered$`Likes (tokenized)`,
                        engine = "hunspell")

evaluation_likes_filtered$`Likes (tokenized)` <-
  evaluation_likes_filtered$`Likes (tokenized)` %>%
  lemmatize_strings(dictionary = lemma_dictionary_for_likes)

View(evaluation_likes_filtered)
lemma_dictionary_for_wishes <-
  make_lemma_dictionary(evaluation_wishes_filtered$`Wishes (tokenized)`,
                        engine = "hunspell")

evaluation_wishes_filtered$`Wishes (tokenized)` <-
  evaluation_wishes_filtered$`Wishes (tokenized)` %>%
  lemmatize_strings(dictionary = lemma_dictionary_for_wishes)

View(evaluation_wishes_filtered)



## Word Count and Top Words

Here, we calculate the words each gender used in the dataset. We then group it according to the class groups available and further according to the likes and wishes. Next, we evaluate the mostly used words by each gender in each class group and group it further according to the likes and wishes.
{r Your Eighth Code Chunk}
## Evaluation Likes ----
### Word count per gender ----
word_count_per_gender_likes <- evaluation_likes_filtered %>%
  group_by(`Student's Gender`) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

word_count_per_gender_likes %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  rename(`Number of Words` = num_words) %>%
  kable("html", escape = FALSE, align = "c",
        caption = "Number of Significant Words in Evaluation Likes 
                   per Gender: Minus contractions, special characters, 
                   stopwords, short words, and censored words.") %>%
  kable_styling(bootstrap_options =
                  c("striped", "condensed", "bordered"),
                full_width = FALSE)

### Word count per group ----
word_count_per_group <- evaluation_likes_filtered %>%
  group_by(`Class Group`) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

word_count_per_group %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  rename(`Number of Words` = num_words) %>%
  kable("html", escape = FALSE, align = "c",
        caption = "Number of Significant Words in Evaluation Likes 
                   per Group: Minus contractions, special characters, 
                   stopwords, short words, and censored words.") %>%
  kable_styling(bootstrap_options =
                  c("striped", "condensed", "bordered"),
                full_width = FALSE)

## Evaluation Wishes ----
### Word count per gender ----
word_count_per_gender_wishes <- evaluation_wishes_filtered %>%
  group_by(`Student's Gender`) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

word_count_per_gender_wishes %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  rename(`Number of Words` = num_words) %>%
  kable("html", escape = FALSE, align = "c",
        caption = "Number of Significant Words in Evaluation Wishes 
                   per Gender: Minus contractions, special characters, 
                   stopwords, short words, and censored words.") %>%
  kable_styling(bootstrap_options =
                  c("striped", "condensed", "bordered"),
                full_width = FALSE)

### Word count per group ----
word_count_per_group_wishes <- evaluation_wishes_filtered %>%
  group_by(`Class Group`) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

word_count_per_group_wishes %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  rename(`Number of Words` = num_words) %>%
  kable("html", escape = FALSE, align = "c",
        caption = "Number of Significant Words in Evaluation Wishes 
                   per Group: Minus contractions, special characters, 
                   stopwords, short words, and censored words.") %>%
  kable_styling(bootstrap_options =
                  c("striped", "condensed", "bordered"),
                full_width = FALSE)

# STEP 7. Top Words ----
## Evaluation Likes ----
### Top 10 words for female students ----
evaluation_likes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  filter(`Student's Gender` == "Female") %>%
  count(`Likes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Likes (tokenized)` = reorder(`Likes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Likes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes for Female
          Students") +
  coord_flip()

### Top 10 words for male students ----
evaluation_likes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  filter(`Student's Gender` == "Male") %>%
  count(`Likes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Likes (tokenized)` = reorder(`Likes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Likes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes for Male
          Students") +
  coord_flip()

### Top 10 words per gender ----
popular_words <- evaluation_likes_filtered %>%
  group_by(`Student's Gender`) %>%
  count(`Likes (tokenized)`, `Student's Gender`, sort = TRUE) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Student's Gender`, n) %>%
  mutate(row = row_number())

popular_words %>%
  ggplot(aes(row, n, fill = `Student's Gender`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation",
       y = "Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes per Gender") +
  facet_wrap(~`Student's Gender`, scales = "free") +
  scale_x_continuous(
                     breaks = popular_words$row,
                     labels = popular_words$`Likes (tokenized)`) +
  coord_flip()

### Top words for Group A students ----
evaluation_likes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  filter(`Class Group` == "A") %>%
  count(`Likes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Likes (tokenized)` = reorder(`Likes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Likes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes for Group A
          Students") +
  coord_flip()

### Top words for Group B students ----
evaluation_likes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  filter(`Class Group` == "B") %>%
  count(`Likes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Likes (tokenized)` = reorder(`Likes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Likes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes for Group B
          Students") +
  coord_flip()

### Top words for Group C students ----
evaluation_likes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  filter(`Class Group` == "C") %>%
  count(`Likes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Likes (tokenized)` = reorder(`Likes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Likes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes for Group C
          Students") +
  coord_flip()

### Top 10 words per group ----
popular_words <- evaluation_likes_filtered %>%
  group_by(`Class Group`) %>%
  count(`Likes (tokenized)`, `Class Group`, sort = TRUE) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Class Group`, n) %>%
  mutate(row = row_number())

popular_words %>%
  ggplot(aes(row, n, fill = `Class Group`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "Number of Times Used") +
  ggtitle("Most Frequently Used Words in Course Evaluation Likes per 
          Class Group") +
  facet_wrap(~`Class Group`, scales = "free") +
  scale_x_continuous(
                     breaks = popular_words$row,
                     labels = popular_words$`Likes (tokenized)`) +
  coord_flip()

## Evaluation Wishes ----
### Top 10 words for female students ----
evaluation_wishes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  filter(`Student's Gender` == "Female") %>%
  count(`Wishes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Wishes (tokenized)` = reorder(`Wishes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Wishes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes for Female
          Students") +
  coord_flip()

### Top 10 words for male students ----
evaluation_wishes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  filter(`Student's Gender` == "Male") %>%
  count(`Wishes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Wishes (tokenized)` = reorder(`Wishes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Wishes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes for Male
          Students") +
  coord_flip()

### Top 10 words per gender ----
popular_words <- evaluation_wishes_filtered %>%
  group_by(`Student's Gender`) %>%
  count(`Wishes (tokenized)`, `Student's Gender`, sort = TRUE) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Student's Gender`, n) %>%
  mutate(row = row_number())

popular_words %>%
  ggplot(aes(row, n, fill = `Student's Gender`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "Number of Times Used") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes per Gender") +
  facet_wrap(~`Student's Gender`, scales = "free") +
  scale_x_continuous(
                     breaks = popular_words$row,
                     labels = popular_words$`Wishes (tokenized)`) +
  coord_flip()

### Top words for Group A students ----
evaluation_wishes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  filter(`Class Group` == "A") %>%
  count(`Wishes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Wishes (tokenized)` = reorder(`Wishes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Wishes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes for Group A
          Students") +
  coord_flip()

### Top words for Group B students ----
evaluation_wishes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  filter(`Class Group` == "B") %>%
  count(`Wishes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Wishes (tokenized)` = reorder(`Wishes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Wishes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes for Group B
          Students") +
  coord_flip()

### Top words for Group C students ----
evaluation_wishes_filtered %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  filter(`Class Group` == "C") %>%
  count(`Wishes (tokenized)`, sort = TRUE) %>%
  top_n(9) %>%
  mutate(`Wishes (tokenized)` = reorder(`Wishes (tokenized)`, n)) %>%
  ggplot() +
  geom_col(aes(`Wishes (tokenized)`, n), fill = blue_grey_colours_1) +
  blue_grey_theme() +
  xlab("Word in Course Evaluation") +
  ylab("Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes for Group C
          Students") +
  coord_flip()

### Top 10 words per group ----
popular_words <- evaluation_wishes_filtered %>%
  group_by(`Class Group`) %>%
  count(`Wishes (tokenized)`, `Class Group`, sort = TRUE) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Class Group`, n) %>%
  mutate(row = row_number())

popular_words %>%
  ggplot(aes(row, n, fill = `Class Group`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation",
       y = "Number of Times Used (Term Frequency)") +
  ggtitle("Most Frequently Used Words in Course Evaluation Wishes per 
          Class Group") +
  facet_wrap(~`Class Group`, scales = "free") +
  scale_x_continuous(
                     breaks = popular_words$row,
                     labels = popular_words$`Wishes (tokenized)`) +
  coord_flip()


## Word Cloud

This is representation of the most appearing words in the dataset in a visually appealing manner. It has disadvantages like: subjectivity, ambiguity, lack of context and statistical limitations.
{r Your Ninth Code Chunk}
## Evaluation Likes ----
evaluation_likes_filtered_cloud <- evaluation_likes_filtered %>%
  count(`Likes (tokenized)`, sort = TRUE)

wordcloud2(evaluation_likes_filtered_cloud, size = .5)

## Evaluation Wishes ----
evaluation_wishes_filtered_cloud <- evaluation_wishes_filtered %>%
  count(`Wishes (tokenized)`, sort = TRUE)

wordcloud2(evaluation_wishes_filtered_cloud, size = .5)


##  Term Frequency - Inverse Document Frequency (TF-IDF)

TF-IDF is used to evaluate the importance of a word in a document relative to a collection of documents.The assumption behind TF-IDF is that terms that appear more frequently in a document (or a comment in this case) should be given a higher weight, unless it also appears in many other documents (or many other comments in this case).

Term Frequency (TF) - This measures how often a word appears in a specific document. Words that
 occur frequently within a document are likely to be important in describing the content of that document.
### Formula for TF ----
(Number of times a term appears in a document) / (Total number of terms in the document)

Inverse Document Frequency (IDF) - This measures how unique or rare a word is across a collection of documents.Words that are rare across many documents but present in one specific document are considered more valuable in distinguishing that document from others.
### Formula for IDF ----
log((Total number of documents in the corpus) / (Number of documents containing the term))


{r Your Tenth Code Chunk}
## Evaluation Likes ----
### TF-IDF Score per Gender ----
popular_tfidf_words_gender_likes <- evaluation_likes_filtered %>%
  unnest_tokens(word, `Likes (tokenized)`) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Likes (tokenized)` = word) %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  count(`Student's Gender`, `Likes (tokenized)`, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(`Likes (tokenized)`, `Student's Gender`, n)

head(popular_tfidf_words_gender_likes)

top_popular_tfidf_words <- popular_tfidf_words_gender_likes %>%
  arrange(desc(tf_idf)) %>%
  mutate(`Likes (tokenized)` =
           factor(`Likes (tokenized)`,
                  levels = rev(unique(`Likes (tokenized)`)))) %>%
  group_by(`Student's Gender`) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Student's Gender`, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, fill = `Student's Gender`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "TF-IDF Score") +
  ggtitle("Important Words using TF-IDF by Chart Level") +
  ggtitle("Most Important Words by TF-IDF Score in Course Evaluation Likes per 
      Class Group") +
  facet_wrap(~`Student's Gender`, scales = "free") +
  scale_x_continuous(
                     breaks = top_popular_tfidf_words$row,
                     labels = top_popular_tfidf_words$`Likes (tokenized)`) +
  coord_flip()

### TF-IDF Score per Group ----
popular_tfidf_words_likes <- evaluation_likes_filtered %>%
  unnest_tokens(word, `Likes (tokenized)`) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Likes (tokenized)` = word) %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Likes (tokenized)`) %>%
  count(`Class Group`, `Likes (tokenized)`, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(`Likes (tokenized)`, `Class Group`, n)

head(popular_tfidf_words_likes)

top_popular_tfidf_words <- popular_tfidf_words_likes %>%
  arrange(desc(tf_idf)) %>%
  mutate(`Likes (tokenized)` =
           factor(`Likes (tokenized)`,
                  levels = rev(unique(`Likes (tokenized)`)))) %>%
  group_by(`Class Group`) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Class Group`, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, fill = `Class Group`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "TF-IDF Score") +
  ggtitle("Important Words using TF-IDF by Chart Level") +
  ggtitle("Most Important Words by TF-IDF Score in Course Evaluation Likes per 
      Class Group") +
  facet_wrap(~`Class Group`, scales = "free") +
  scale_x_continuous(
                     breaks = top_popular_tfidf_words$row,
                     labels = top_popular_tfidf_words$`Likes (tokenized)`) +
  coord_flip()

## Evaluation Wishes ----
### TF-IDF Score per Gender ----
popular_tfidf_words_gender_wishes <- evaluation_wishes_filtered %>%
  unnest_tokens(word, `Wishes (tokenized)`) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Wishes (tokenized)` = word) %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  count(`Student's Gender`, `Wishes (tokenized)`, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(`Wishes (tokenized)`, `Student's Gender`, n)

head(popular_tfidf_words_gender_wishes)

top_popular_tfidf_words <- popular_tfidf_words_gender_wishes %>%
  arrange(desc(tf_idf)) %>%
  mutate(`Wishes (tokenized)` =
           factor(`Wishes (tokenized)`,
                  levels = rev(unique(`Wishes (tokenized)`)))) %>%
  group_by(`Student's Gender`) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Student's Gender`, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, fill = `Student's Gender`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "TF-IDF Score") +
  ggtitle("Important Words using TF-IDF by Chart Level") +
  ggtitle("Most Important Words by TF-IDF Score in Course Evaluation Wishes per 
      Class Group") +
  facet_wrap(~`Student's Gender`, scales = "free") +
  scale_x_continuous(
                     breaks = top_popular_tfidf_words$row,
                     labels = top_popular_tfidf_words$`Wishes (tokenized)`) +
  coord_flip()

### TF-IDF Score per Group ----
popular_tfidf_words_likes <- evaluation_wishes_filtered %>%
  unnest_tokens(word, `Wishes (tokenized)`) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  rename(`Wishes (tokenized)` = word) %>%
  select(`Class Group`, `Student's Gender`,
         `Average Course Evaluation Rating`, `Wishes (tokenized)`) %>%
  count(`Class Group`, `Wishes (tokenized)`, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(`Wishes (tokenized)`, `Class Group`, n)

head(popular_tfidf_words_likes)

top_popular_tfidf_words <- popular_tfidf_words_likes %>%
  arrange(desc(tf_idf)) %>%
  mutate(`Wishes (tokenized)` =
           factor(`Wishes (tokenized)`,
                  levels = rev(unique(`Wishes (tokenized)`)))) %>%
  group_by(`Class Group`) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(`Class Group`, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, fill = `Class Group`)) +
  geom_col(fill = blue_grey_colours_1) +
  blue_grey_theme() +
  labs(x = "Word in Course Evaluation", y = "TF-IDF Score") +
  ggtitle("Important Words using TF-IDF by Chart Level") +
  ggtitle("Most Important Words by TF-IDF Score in Course Evaluation Wishes per 
      Class Group") +
  facet_wrap(~`Class Group`, scales = "free") +
  scale_x_continuous(
                     breaks = top_popular_tfidf_words$row,
                     labels = top_popular_tfidf_words$`Wishes (tokenized)`) +
  coord_flip()


## References
Ashton, D., Porter, S., library), N. D. (chart js, library), T. L. (chart js, & library), W. E. (chart js. (2016). radarchart: Radar Chart from â€˜Chart.jsâ€™ (0.3.1) [Computer software]. https://cran.r-project.org/package=radarchart

Auguie, B., & Antonov, A. (2017). gridExtra: Miscellaneous Functions for â€˜Gridâ€™ Graphics (2.3) [Computer software]. https://cran.r-project.org/package=gridExtra

Bevans, R. (2023b). Sample Crop Data Dataset for ANOVA (Version 1) [Dataset]. Scribbr. https://www.scribbr.com/wp-content/uploads//2020/03/crop.data_.anova_.zip 

CsÃ¡rdi, G., Nepusz, T., Traag, V., HorvÃ¡t, S., Zanini, F., Noom, D., MÃ¼ller, K., Salmon, M., & details, C. Z. I. igraph author. (2023). igraph: Network Analysis and Visualization (1.5.1) [Computer software]. https://cran.r-project.org/package=igraph 

Gu, Z., Gu, L., Eils, R., Schlesner, M., & Brors, B. (2014). Circlize Implements and Enhances Circular Visualization in R. Bioinformatics (Oxford, England), 30(19), 2811â€“2812. https://doi.org/10.1093/bioinformatics/btu393

Gu, Z. (2022). circlize: Circular Visualization (0.4.15) [Computer software]. https://cran.r-project.org/package=circlize 

Lang, D., & Chien, G. (2018). wordcloud2: Create Word Cloud by â€˜htmlwidgetâ€™ (0.2.1) [Computer software]. https://cran.r-project.org/package=wordcloud2 

Leonawicz, M. (2023). memery: Internet Memes for Data Analysts (0.5.7) [Computer software]. https://cran.r-project.org/package=memery 

Liske, D. (2018). R NLP & Machine Learning: Lyric Analysis [Tutorial]. Datacamp. https://www.datacamp.com/tutorial/R-nlp-machine-learning 

Ooms, J. (2023). magick: Advanced Graphics and Image-Processing in R (2.7.5) [Computer software]. https://cran.r-project.org/package=magick 

Pedersen, T. L., & RStudio. (2022). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks (2.1.0) [Computer software]. https://cran.r-project.org/package=ggraph 

Phillips, N. (2017). yarrr: A Companion to the e-Book â€˜YaRrr!: The Pirateâ€™s Guide to Râ€™ (0.1.5) [Computer software]. https://cran.r-project.org/package=yarrr

Queiroz, G. D., Fay, C., Hvitfeldt, E., Keyes, O., Misra, K., Mastny, T., Erickson, J., Robinson, D., Silge  [aut, J., & cre. (2023). tidytext: Text Mining using â€˜dplyrâ€™, â€˜ggplot2â€™, and Other Tidy Tools (0.4.1) [Computer software]. https://cran.r-project.org/package=tidytext 

Ren, K., & Russell, K. (2021). formattable: Create â€˜Formattableâ€™ Data Structures (0.2.1) [Computer software]. https://cran.r-project.org/package=formattable

Robinson, D., Misra, K., Silge  [aut, J., & cre. (2022). widyr: Widen, Process, then Re-Tidy Data (0.1.5) [Computer software]. https://cran.r-project.org/package=widyr 

Slowikowski, K., Schep, A., Hughes, S., Dang, T. K., Lukauskas, S., Irisson, J.-O., Kamvar, Z. N., Ryan, T., Christophe, D., Hiroaki, Y., Gramme, P., Abdol, A. M., Barrett, M., Cannoodt, R., Krassowski, M., Chirico, M., & Aphalo, P. (2023). ggrepel: Automatically Position Non-Overlapping Text Labels with â€˜ggplot2â€™ (0.9.3) [Computer software]. https://cran.r-project.org/package=ggrepel 

Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., Dunnington, D., Posit, & PBC. (2023). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics (3.4.3) [Computer software]. https://cran.r-project.org/package=ggplot2 

Wickham, H., FranÃ§ois, R., Henry, L., MÃ¼ller, K., Vaughan, D., Software, P., & PBC. (2023). dplyr: A Grammar of Data Manipulation (1.1.3) [Computer software]. https://cran.r-project.org/package=dplyr 

Wickham, H., Vaughan, D., Girlich, M., Ushey, K., Posit, & PBC. (2023). tidyr: Tidy Messy Data (1.3.0) [Computer software]. https://cran.r-project.org/package=tidyr 

 Xie  [aut, Y., cre, Sarma, A., Vogt, A., Andrew, A., Zvoleff, A., Al-Zubaidi, A., http://www.andre-simon.de), A. S. (the C. files under inst/themes/ were derived from the H. package, Atkins, A., Wolen, A., Manton, A., Yasumoto, A., Baumer, B., Diggs, B., Zhang, B., Yapparov, B., Pereira, C., Dervieux, C., Hall, D., â€¦ PBC. (2023). knitr: A General-Purpose Package for Dynamic Report Generation in R (1.44) [Computer software]. https://cran.r-project.org/package=knitr 
 
Zhu  [aut, H., cre, Travison, T., Tsai, T., Beasley, W., Xie, Y., Yu, G., Laurent, S., Shepherd, R., Sidi, Y., Salzer, B., Gui, G., Fan, Y., Murdoch, D., & Evans, B. (2021). kableExtra: Construct Complex Table with â€˜kableâ€™ and Pipe Syntax (1.3.4) [Computer software]. https://cran.r-project.org/package=kableExtra 

*etc.* as per the lab submission requirements. Be neat and communicate in a clear and logical manner.